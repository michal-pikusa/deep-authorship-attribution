{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepNeuralText\n",
    "\n",
    "A multi-class text classification based on a deep recurrent neural network (RNN) with Long Short Term Memory (LSTM) units. The network includes an embedding layer, so the input is first transformed into padded sequences of fixed length. PoS-tagging is also included to improve classification, but is not essential for the network to run.\n",
    "\n",
    "Version: 0.2\n",
    "\n",
    "Author: Michal Pikusa (pikusa.michal@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, GlobalMaxPool1D, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading the data. The input text file contains a text to classify in each line, so the function returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(infile):\n",
    "    text_file = open(infile,'r')\n",
    "    text = text_file.readlines()\n",
    "    text = list(map(str.strip,text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Parts-of-Speech tagging with NLTK. Each line of the input is first tokenized and then tagged. Resulting tag is combined with the token to create a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(docs):\n",
    "    tagged_sentences = []\n",
    "    for item in docs:\n",
    "        combined = []\n",
    "        tokenized = nltk.word_tokenize(item)\n",
    "        tagged = nltk.pos_tag(tokenized)\n",
    "        for i in range(len(tagged)):\n",
    "            combined.append('_'.join([tagged[i][0], tagged[i][1]]))\n",
    "        combined_string = ' '.join(combined)\n",
    "        tagged_sentences.append(combined_string)\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for encoding the data. Each line of input is tokenized to create a vocabulary with an upper limit of 20,000 words. Then, sequences of vocabulary indices are created with a fixed length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(docs):\n",
    "    tokenizer = text.Tokenizer(num_words=20000)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    tokenized = tokenizer.texts_to_sequences(docs)\n",
    "    docs = sequence.pad_sequences(tokenized, maxlen=128)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for building the model with Keras. Main function is embedded into another function to accomodate passing the model to a scikit-keras wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embed_size,max_length,vocab_size):\n",
    "    def build_model():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embed_size, input_length=max_length))\n",
    "        model.add(Conv1D(embed_size, 7, activation='relu', padding='same'))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(LSTM(50, return_sequences=True))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(3, activation=\"sigmoid\"))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_data('corpus.txt')\n",
    "labels = load_data('labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag it (it can be commented out to improve speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pos_tag(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode it, make sure to have all the data as integers, and dummy encode the labels to fit the classes into the output layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_encoded = encode_data(docs)\n",
    "docs_encoded = np.array(docs_encoded)\n",
    "labels = np.array(labels)\n",
    "train_data = docs_encoded.astype(int)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "encoded_labels = encoder.transform(labels)\n",
    "train_labels = np_utils.to_categorical(encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the results with 10-fold cross-validation, and print the resulting accuracy and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 70s 4ms/step - loss: 0.4512 - acc: 0.7869\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 71s 4ms/step - loss: 0.1874 - acc: 0.9270\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 72s 4ms/step - loss: 0.0977 - acc: 0.9628\n",
      "1958/1958 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 68s 4ms/step - loss: 0.4718 - acc: 0.7703\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 71s 4ms/step - loss: 0.2036 - acc: 0.9198\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 71s 4ms/step - loss: 0.0998 - acc: 0.9628\n",
      "1958/1958 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 68s 4ms/step - loss: 0.4530 - acc: 0.7857\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 72s 4ms/step - loss: 0.1913 - acc: 0.9257\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 72s 4ms/step - loss: 0.0974 - acc: 0.9643\n",
      "1958/1958 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 70s 4ms/step - loss: 0.4317 - acc: 0.7942\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 73s 4ms/step - loss: 0.1807 - acc: 0.9287\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 72s 4ms/step - loss: 0.0932 - acc: 0.9656\n",
      "1958/1958 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 68s 4ms/step - loss: 0.4279 - acc: 0.7997\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 71s 4ms/step - loss: 0.1789 - acc: 0.9300\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 71s 4ms/step - loss: 0.0912 - acc: 0.9640\n",
      "1958/1958 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 68s 4ms/step - loss: 0.4143 - acc: 0.8087\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 72s 4ms/step - loss: 0.1750 - acc: 0.9321\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 72s 4ms/step - loss: 0.0905 - acc: 0.9663\n",
      "1958/1958 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 69s 4ms/step - loss: 0.4370 - acc: 0.7918\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 72s 4ms/step - loss: 0.1812 - acc: 0.9284\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 72s 4ms/step - loss: 0.0949 - acc: 0.9643\n",
      "1958/1958 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 69s 4ms/step - loss: 0.4343 - acc: 0.7948\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 73s 4ms/step - loss: 0.1808 - acc: 0.9284\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 77s 4ms/step - loss: 0.0921 - acc: 0.9656\n",
      "1958/1958 [==============================] - 3s 1ms/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 85s 5ms/step - loss: 0.4436 - acc: 0.7886\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 88s 5ms/step - loss: 0.1869 - acc: 0.9240\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 75s 4ms/step - loss: 0.0937 - acc: 0.9636\n",
      "1958/1958 [==============================] - 2s 1ms/step\n",
      "Epoch 1/3\n",
      "17622/17622 [==============================] - 73s 4ms/step - loss: 0.4509 - acc: 0.7785\n",
      "Epoch 2/3\n",
      "17622/17622 [==============================] - 74s 4ms/step - loss: 0.1834 - acc: 0.9290\n",
      "Epoch 3/3\n",
      "17622/17622 [==============================] - 76s 4ms/step - loss: 0.0933 - acc: 0.9650\n",
      "1957/1957 [==============================] - 2s 1ms/step\n",
      "Baseline: 89.03% (0.48%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=build_model(128,128,20000),epochs=3,batch_size=32,verbose=1)\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=128)\n",
    "results = cross_val_score(estimator=estimator,X=train_data,y=train_labels,cv=folds)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
