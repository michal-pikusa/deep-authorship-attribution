{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepNeuralText\n",
    "\n",
    "A multi-class text classification based on a deep recurrent neural network (RNN) with Long Short Term Memory (LSTM) units. The network includes an embedding layer, so the input is first transformed into padded sequences of fixed length. PoS-tagging is also included to improve classification, but is not essential for the network to run.\n",
    "\n",
    "Version: 0.2\n",
    "\n",
    "Author: Michal Pikusa (pikusa.michal@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading the data. The input text file contains a text to classify in each line, so the function returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(infile):\n",
    "    text_file = open(infile,'r')\n",
    "    text = text_file.readlines()\n",
    "    text = list(map(str.strip,text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Parts-of-Speech tagging with NLTK. Each line of the input is first tokenized and then tagged. Resulting tag is combined with the token to create a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(docs):\n",
    "    tagged_sentences = []\n",
    "    for item in docs:\n",
    "        combined = []\n",
    "        tokenized = nltk.word_tokenize(item)\n",
    "        tagged = nltk.pos_tag(tokenized)\n",
    "        for i in range(len(tagged)):\n",
    "            combined.append('_'.join([tagged[i][0], tagged[i][1]]))\n",
    "        combined_string = ' '.join(combined)\n",
    "        tagged_sentences.append(combined_string)\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for encoding the data. Each line of input is tokenized to create a vocabulary with an upper limit of 20,000 words. Then, sequences of vocabulary indices are created with a fixed length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(docs):\n",
    "    tokenizer = text.Tokenizer(num_words=20000)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    tokenized = tokenizer.texts_to_sequences(docs)\n",
    "    docs = sequence.pad_sequences(tokenized, maxlen=128)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for building the model with Keras. Main function is embedded into another function to accomodate passing the model to a scikit-keras wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embed_size,max_length,vocab_size):\n",
    "    def build_model():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embed_size, input_length=max_length))\n",
    "        model.add(LSTM(50, return_sequences=True))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(3, activation=\"sigmoid\"))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_data('corpus.txt')\n",
    "labels = load_data('labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag it (it can be commented out to improve speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pos_tag(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode it, make sure to have all the data as integers, and dummy encode the labels to fit the classes into the output layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_encoded = encode_data(docs)\n",
    "docs_encoded = np.array(docs_encoded)\n",
    "labels = np.array(labels)\n",
    "train_data = docs_encoded.astype(int)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels)\n",
    "encoded_labels = encoder.transform(labels)\n",
    "train_labels = np_utils.to_categorical(encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the results with 10-fold cross-validation, and print the resulting accuracy and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 45s 3ms/step - loss: 0.5877 - acc: 0.6957\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 49s 3ms/step - loss: 0.3035 - acc: 0.8733\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 49s 3ms/step - loss: 0.1586 - acc: 0.9383\n",
      "1958/1958 [==============================] - 1s 482us/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 44s 3ms/step - loss: 0.5574 - acc: 0.7162\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 51s 3ms/step - loss: 0.2791 - acc: 0.8839\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 52s 3ms/step - loss: 0.1516 - acc: 0.9413\n",
      "1958/1958 [==============================] - 1s 491us/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 45s 3ms/step - loss: 0.5218 - acc: 0.7431\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 52s 3ms/step - loss: 0.2583 - acc: 0.8943\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 51s 3ms/step - loss: 0.1414 - acc: 0.9471\n",
      "1958/1958 [==============================] - 1s 496us/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 45s 3ms/step - loss: 0.5087 - acc: 0.7512\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 48s 3ms/step - loss: 0.2373 - acc: 0.9053\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 52s 3ms/step - loss: 0.1266 - acc: 0.9519\n",
      "1958/1958 [==============================] - 1s 504us/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 45s 3ms/step - loss: 0.5855 - acc: 0.6973\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 49s 3ms/step - loss: 0.3575 - acc: 0.8419\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 49s 3ms/step - loss: 0.1911 - acc: 0.9249\n",
      "1958/1958 [==============================] - 1s 491us/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 45s 3ms/step - loss: 0.4947 - acc: 0.7551\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 75s 4ms/step - loss: 0.2197 - acc: 0.9115\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 49s 3ms/step - loss: 0.1112 - acc: 0.9577\n",
      "1958/1958 [==============================] - 1s 498us/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 48s 3ms/step - loss: 0.5440 - acc: 0.7247\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 51s 3ms/step - loss: 0.2652 - acc: 0.8930\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 52s 3ms/step - loss: 0.1522 - acc: 0.9409\n",
      "1958/1958 [==============================] - 1s 500us/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 46s 3ms/step - loss: 0.4733 - acc: 0.7702\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 49s 3ms/step - loss: 0.2135 - acc: 0.9160\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 49s 3ms/step - loss: 0.1138 - acc: 0.9573\n",
      "1958/1958 [==============================] - 1s 506us/step\n",
      "Epoch 1/3\n",
      "17621/17621 [==============================] - 45s 3ms/step - loss: 0.5755 - acc: 0.7037\n",
      "Epoch 2/3\n",
      "17621/17621 [==============================] - 50s 3ms/step - loss: 0.3080 - acc: 0.8731\n",
      "Epoch 3/3\n",
      "17621/17621 [==============================] - 49s 3ms/step - loss: 0.1569 - acc: 0.9391\n",
      "1958/1958 [==============================] - 1s 502us/step\n",
      "Epoch 1/3\n",
      "17622/17622 [==============================] - 46s 3ms/step - loss: 0.5583 - acc: 0.7206\n",
      "Epoch 2/3\n",
      "17622/17622 [==============================] - 49s 3ms/step - loss: 0.2761 - acc: 0.8884\n",
      "Epoch 3/3\n",
      "17622/17622 [==============================] - 49s 3ms/step - loss: 0.1438 - acc: 0.9467\n",
      "1957/1957 [==============================] - 1s 522us/step\n",
      "Baseline: 88.61% (0.63%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=build_model(128,128,20000),epochs=3,batch_size=32,verbose=1)\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=128)\n",
    "results = cross_val_score(estimator=estimator,X=train_data,y=train_labels,cv=folds)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
