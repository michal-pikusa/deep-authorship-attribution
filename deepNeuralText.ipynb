{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepNeuralText\n",
    "\n",
    "A multi-class text classification based on a deep recurrent neural network (RNN) with Long Short Term Memory (LSTM) units. The network includes an embedding layer, so the input is first transformed into padded sequences of fixed length. PoS-tagging is also included to improve classification, but is not essential for the network to run.\n",
    "\n",
    "Version: 0.2\n",
    "\n",
    "Author: Michal Pikusa (pikusa.michal@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for loading the data. The input text file contains a text to classify in each line, so the function returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(infile):\n",
    "    text_file = open(infile,'r')\n",
    "    text = text_file.readlines()\n",
    "    text = list(map(str.strip,text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Parts-of-Speech tagging with NLTK. Each line of the input is first tokenized and then tagged. Resulting tag is combined with the token to create a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(docs):\n",
    "    tagged_sentences = []\n",
    "    for item in docs:\n",
    "        combined = []\n",
    "        tokenized = nltk.word_tokenize(item)\n",
    "        tagged = nltk.pos_tag(tokenized)\n",
    "        for i in range(len(tagged)):\n",
    "            combined.append('_'.join([tagged[i][0], tagged[i][1]]))\n",
    "        combined_string = ' '.join(combined)\n",
    "        tagged_sentences.append(combined_string)\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for encoding the data. Each line of input is tokenized to create a vocabulary with an upper limit of 20,000 words. Then, sequences of vocabulary indices are created with a fixed length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(docs):\n",
    "    tokenizer = text.Tokenizer(num_words=20000)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    tokenized = tokenizer.texts_to_sequences(docs)\n",
    "    docs = sequence.pad_sequences(tokenized, maxlen=128)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for building the model with Keras. Main function is embedded into another function to accomodate passing the model to a scikit-keras wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embed_size,max_length,vocab_size):\n",
    "    def build_model():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embed_size, input_length=max_length))\n",
    "        model.add(LSTM(50, return_sequences=True))\n",
    "        model.add(GlobalMaxPool1D())\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    return build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_data('corpus.txt')\n",
    "labels = load_data('labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag it (it can be commented out to improve speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pos_tag(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode it, make sure to have all the data and labels as integers, and finally reshapre the labels to fit them into the network output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_encoded = encode_data(docs)\n",
    "docs_encoded = np.array(docs_encoded)\n",
    "labels_encoded = np.array(labels)\n",
    "labels_encoded = labels_encoded.astype(int)\n",
    "train_data = docs_encoded.astype(int)\n",
    "train_labels = labels_encoded.astype(int)\n",
    "train_labels = train_labels.reshape(len(train_labels), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the results with 10-fold cross-validation, and print the resulting accuracy and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 85.77% (1.07%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=build_model(128,128,20000),epochs=3,batch_size=32,verbose=0)\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=128)\n",
    "results = cross_val_score(estimator=estimator,X=train_data,y=train_labels,cv=folds)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
